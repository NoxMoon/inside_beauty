{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging to Price\n",
    "\n",
    "Cosmetic products' packaging certainly have an influence on their price. Although it could be hard to train a neural network with \"sense of beauty\" when we only have 6000 images of products, it is worth trying to see how much information we can extract from these pictures...\n",
    "\n",
    "#### Define neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        \n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        \n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        \n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        \n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(-1)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define training and prediction functions\n",
    "The training function has features include:\n",
    "* batch gradient decent\n",
    "* evaluate on validation data\n",
    "* early-stopping\n",
    "* load best weights from best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "def train(train_loader, val_loader=None, model=None, epoch=1, optimizer=None, criterion=None, early_stopping=3):\n",
    "        \n",
    "    best_loss = 10000000\n",
    "    best_epoch = 0\n",
    "    best_model_wts = None\n",
    "    \n",
    "    for t in range(epoch):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for i_batch, batch in tqdm(enumerate(train_loader)):\n",
    "            \n",
    "            batch_X, batch_y = batch\n",
    "            batch_y_pred = model(batch_X)\n",
    "            loss = criterion(batch_y_pred, batch_y.type(torch.FloatTensor))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if val_loader is not None: #evaluate on validation data\n",
    "            \n",
    "            model.eval()\n",
    "            running_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i_batch, batch in enumerate(val_loader):\n",
    "                    batch_X, batch_y = batch\n",
    "                    batch_y_pred = model(batch_X)\n",
    "                    loss = criterion(batch_y_pred, batch_y.type(torch.FloatTensor))                   \n",
    "                    running_loss += loss.item() * batch_X.size()[0]\n",
    "        \n",
    "            epoch_loss = running_loss / len(val_loader.dataset)\n",
    "            print (\"epoch %d, loss %.6f\"%(t, epoch_loss))\n",
    "            \n",
    "            if epoch_loss < best_loss: #keep track of best loss and epoch\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = t\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            if (t - best_epoch > early_stopping): #early stopping if loss haven't improve for n=early_stopping rounds\n",
    "                break\n",
    "                \n",
    "    if best_model_wts is not None:\n",
    "        print(\"load best weights from epoch %d\"%best_epoch)\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        \n",
    "        \n",
    "def predict(dataloader, model):\n",
    "    \n",
    "    N = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    batch_size = dataloader.batch_size\n",
    "    predictions = torch.zeros(N)\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i_batch, batch in tqdm(enumerate(dataloader)):\n",
    "            batch_X, batch_y = batch\n",
    "            batch_y_pred = model(batch_X)\n",
    "                \n",
    "            start = i_batch * batch_size\n",
    "            end = start + batch_size\n",
    "            if i_batch == n_batches - 1:\n",
    "                end = N\n",
    "            predictions[start:end] = batch_y_pred   \n",
    "                \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ImagesWithPaths(Dataset):\n",
    "# dataset initialized with given image paths and corresponding labels\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        assert (len(image_paths)==len(labels))\n",
    "        self.paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        path = self.paths[index]\n",
    "        image = io.imread(path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize train_test_split and KFold\n",
    "\n",
    "In our dataset, same product of different product category are recorded in multiple rows. We should avoid putting these rows in different folds because that may introduce leakage during training and over results would be over-optimistic. Thus we would do train-test split and KFold on unique products, and map the fold number back to the original dataset. This way in the original dataset, the testset ratio may be slightly off what we want originally, and the folds may also be non-equal sized, but this should not be a big problem.\n",
    "\n",
    "Do train-test split and KFold on all unique products. \"-1\" indicate test set, 0-5 indicate the fold in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1554\n",
       " 0    1244\n",
       " 3    1243\n",
       " 2    1243\n",
       " 1    1243\n",
       " 4    1243\n",
       "Name: fold, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "# read data\n",
    "df = pd.read_csv(\"../data_cleaning/images.csv\")\n",
    "df['price'] = df['price'].apply(lambda x: x.replace(',','')).astype('float')\n",
    "df = df.loc[df['price']<300]\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "# get unique products\n",
    "folds_df = df[['product_names','brand']].drop_duplicates()\n",
    "train_folds_df, test_folds_df = train_test_split(folds_df, test_size=0.2, shuffle=True, random_state=777)\n",
    "\n",
    "test_folds_df['fold'] = -1 # test\n",
    "train_folds_df['fold'] = 0\n",
    "n_folds = 5\n",
    "folds = KFold(n_folds, shuffle=True, random_state=777)\n",
    "for i, (trn_idx, val_idx) in enumerate(folds.split(train_folds_df)):\n",
    "    train_folds_df['fold'].iloc[val_idx] = i\n",
    "\n",
    "folds_df = train_folds_df.append(test_folds_df).set_index(['product_names','brand'])\n",
    "folds_df['fold'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map fold number in original dataset, check that the fold sizes are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total fold count\n",
      "-1    1858\n",
      " 1    1490\n",
      " 2    1472\n",
      " 4    1470\n",
      " 0    1469\n",
      " 3    1467\n",
      "Name: fold, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_names</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>image_path</th>\n",
       "      <th>islogo</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clear Complexion Spot Treatment</td>\n",
       "      <td>Merle Norman</td>\n",
       "      <td>20.00</td>\n",
       "      <td>images/skin_care/Clear-Complexion-Spot-Treatme...</td>\n",
       "      <td>0.963436</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acne Solutions Emergency Gel Lotion</td>\n",
       "      <td>Clinique</td>\n",
       "      <td>17.00</td>\n",
       "      <td>images/skin_care/Acne-Solutions-Emergency-Gel-...</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RESIST Daily Pore-Refining Solution 2% BHA</td>\n",
       "      <td>Paula's Choice Skincare</td>\n",
       "      <td>33.00</td>\n",
       "      <td>images/skin_care/RESIST-Daily-Pore-Refining-So...</td>\n",
       "      <td>0.567055</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Max Complexion Correction Pads</td>\n",
       "      <td>Peter Thomas Roth</td>\n",
       "      <td>40.00</td>\n",
       "      <td>images/skin_care/Max-Complexion-Correction-Pad...</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naturals Acne Spot Treatment</td>\n",
       "      <td>Neutrogena</td>\n",
       "      <td>8.49</td>\n",
       "      <td>images/skin_care/Naturals-Acne-Spot-Treatment_...</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                product_names                    brand  price  \\\n",
       "0             Clear Complexion Spot Treatment             Merle Norman  20.00   \n",
       "1         Acne Solutions Emergency Gel Lotion                 Clinique  17.00   \n",
       "2  RESIST Daily Pore-Refining Solution 2% BHA  Paula's Choice Skincare  33.00   \n",
       "3              Max Complexion Correction Pads        Peter Thomas Roth  40.00   \n",
       "4                Naturals Acne Spot Treatment               Neutrogena   8.49   \n",
       "\n",
       "                                          image_path    islogo  fold  \n",
       "0  images/skin_care/Clear-Complexion-Spot-Treatme...  0.963436     2  \n",
       "1  images/skin_care/Acne-Solutions-Emergency-Gel-...  0.011180     0  \n",
       "2  images/skin_care/RESIST-Daily-Pore-Refining-So...  0.567055    -1  \n",
       "3  images/skin_care/Max-Complexion-Correction-Pad...  0.005085    -1  \n",
       "4  images/skin_care/Naturals-Acne-Spot-Treatment_...  0.000562     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dfmap(x, series):\n",
    "    return series.loc[(x[0],x[1])]\n",
    "df['fold'] = df[['product_names','brand']].apply(dfmap, series=folds_df['fold'], axis=1)\n",
    "\n",
    "print('total fold count')\n",
    "print(df['fold'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will only train on non-logo images (which we have filtered previously in data cleaning). Check that the folds with non-logo image samples are still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold count for non-logo image samples\n",
      "-1    1282\n",
      " 0    1026\n",
      " 4    1013\n",
      " 1    1004\n",
      " 2    1000\n",
      " 3     999\n",
      "Name: fold, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_names</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>image_path</th>\n",
       "      <th>islogo</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acne Solutions Emergency Gel Lotion</td>\n",
       "      <td>Clinique</td>\n",
       "      <td>17.00</td>\n",
       "      <td>../data_cleaning/images/skin_care/Acne-Solutio...</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Max Complexion Correction Pads</td>\n",
       "      <td>Peter Thomas Roth</td>\n",
       "      <td>40.00</td>\n",
       "      <td>../data_cleaning/images/skin_care/Max-Complexi...</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naturals Acne Spot Treatment</td>\n",
       "      <td>Neutrogena</td>\n",
       "      <td>8.49</td>\n",
       "      <td>../data_cleaning/images/skin_care/Naturals-Acn...</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Benzoyl Peroxide 10%</td>\n",
       "      <td>Jan Marini Skin Research, Inc.</td>\n",
       "      <td>30.00</td>\n",
       "      <td>../data_cleaning/images/skin_care/Benzoyl-Pero...</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Blackhead Dissolving Gel</td>\n",
       "      <td>Proactiv &amp; Proactiv+</td>\n",
       "      <td>25.00</td>\n",
       "      <td>../data_cleaning/images/skin_care/Blackhead-Di...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         product_names                           brand  price  \\\n",
       "1  Acne Solutions Emergency Gel Lotion                        Clinique  17.00   \n",
       "3       Max Complexion Correction Pads               Peter Thomas Roth  40.00   \n",
       "4         Naturals Acne Spot Treatment                      Neutrogena   8.49   \n",
       "6                 Benzoyl Peroxide 10%  Jan Marini Skin Research, Inc.  30.00   \n",
       "9             Blackhead Dissolving Gel            Proactiv & Proactiv+  25.00   \n",
       "\n",
       "                                          image_path    islogo  fold  \n",
       "1  ../data_cleaning/images/skin_care/Acne-Solutio...  0.011180     0  \n",
       "3  ../data_cleaning/images/skin_care/Max-Complexi...  0.005085    -1  \n",
       "4  ../data_cleaning/images/skin_care/Naturals-Acn...  0.000562     1  \n",
       "6  ../data_cleaning/images/skin_care/Benzoyl-Pero...  0.005258     0  \n",
       "9  ../data_cleaning/images/skin_care/Blackhead-Di...  0.007812     3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df = df.loc[~df['image_path'].isnull()]\n",
    "image_df['image_path'] = image_df['image_path'].apply(lambda x: '../data_cleaning/'+x)\n",
    "image_df = image_df.loc[image_df['islogo']<0.5].drop_duplicates()\n",
    "print('fold count for non-logo image samples')\n",
    "print(image_df['fold'].value_counts())\n",
    "image_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "For training set, we will save the out-of-bag predictions from K-Fold cross validation. For test set, we will use the average prediction of all five folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_size: 1282\n",
      "fold 1, train size: 5298, val size: 1026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1427.891165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1395.720558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 1307.898188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 1215.874073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 1208.373418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 1209.155855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 1205.544970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss 1198.707761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss 1206.425349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss 1198.434764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 1186.935790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss 1194.266859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss 1187.170120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss 1185.395041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, loss 1190.747208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, loss 1183.340459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:01,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, loss 1202.422673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, loss 1193.135762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, loss 1181.878339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, loss 1190.131306\n",
      "load best weights from epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:13,  1.30it/s]\n",
      "21it [00:16,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2, train size: 5320, val size: 1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1377.415144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:03,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1440.801821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 1396.740504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:03,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 1347.266137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:03,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 1291.657711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:03,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 1326.198814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:03,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 1232.905001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:04,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss 1218.068780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:04,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss 1214.924497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:04,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss 1213.428562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:04,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 1213.724607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:04,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss 1215.423198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:04,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss 1224.485352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:04,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss 1223.531747\n",
      "load best weights from epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:12,  1.24it/s]\n",
      "21it [00:16,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3, train size: 5324, val size: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1531.923148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1485.431055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 1384.011502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 1388.313506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 1341.704906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 1357.664564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 1314.443179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss 1307.203820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss 1304.157524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss 1299.830543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 1349.562597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss 1302.341422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss 1292.239661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss 1326.883739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, loss 1314.537301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, loss 1288.219016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:01,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, loss 1283.211544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, loss 1280.029984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, loss 1282.739866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, loss 1285.579555\n",
      "load best weights from epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:12,  1.29it/s]\n",
      "21it [00:15,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4, train size: 5325, val size: 999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1306.268710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1242.975226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:03,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 1205.654553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 1175.085718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 1154.717341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 1144.386506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 1149.005230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss 1119.094392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:03,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss 1147.674136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss 1126.588417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 1126.274168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [02:02,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss 1125.208251\n",
      "load best weights from epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:12,  1.28it/s]\n",
      "21it [00:15,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5, train size: 5311, val size: 1013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "83it [02:02,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1464.261156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1416.442417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:03,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 1329.358447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 1251.269454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 1241.233798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:03,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 1240.503068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 1265.342350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss 1255.099732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:03,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss 1242.932553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:02,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss 1245.694845\n",
      "load best weights from epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:12,  1.25it/s]\n",
      "21it [00:16,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = image_df.loc[(image_df['fold']==-1)]\n",
    "dataset_test = ImagesWithPaths(test_df['image_path'].values, \n",
    "                               test_df['price'].values, \n",
    "                               transform=transforms.ToTensor()) \n",
    "test_loader = DataLoader(dataset_test, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "oof_preds = pd.DataFrame()\n",
    "test_preds = pd.DataFrame({'image_path':test_df['image_path'].values, \n",
    "                           'predict_price':np.zeros([len(test_loader.dataset)])})\n",
    "print('test_size:', len(test_loader.dataset))\n",
    "\n",
    "for i in range(n_folds):\n",
    "    train_df = image_df.loc[(image_df['fold']!=i)]\n",
    "    val_df = image_df.loc[(image_df['fold']==i)]\n",
    "    \n",
    "    #dataset\n",
    "    dataset_train = ImagesWithPaths(train_df['image_path'].values, \n",
    "                                    train_df['price'].values, \n",
    "                                    transform=transforms.ToTensor()) \n",
    "    train_loader = DataLoader(dataset_train, batch_size=64, shuffle=False, num_workers=8)\n",
    "    \n",
    "    #dataloader\n",
    "    dataset_val = ImagesWithPaths(val_df['image_path'].values, \n",
    "                                  val_df['price'].values, \n",
    "                                  transform=transforms.ToTensor()) \n",
    "    val_loader = DataLoader(dataset_val, batch_size=64, shuffle=False, num_workers=8)\n",
    "    \n",
    "    print(\"fold %d, train size: %d, val size: %d\"%(i+1, len(train_loader.dataset), len(val_loader.dataset)))\n",
    "    \n",
    "    #model, optimizer, loss and training\n",
    "    model = Net()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "    train(train_loader, val_loader, model, epoch=20, optimizer=optimizer, criterion=criterion)\n",
    "    \n",
    "    #predict on validation set\n",
    "    predictions = predict(val_loader, model)\n",
    "    oof_preds = oof_preds.append(pd.DataFrame({'image_path':val_df['image_path'].values, \n",
    "                                               'predict_price':predictions.numpy()}))\n",
    "    #predict on test set\n",
    "    predictions = predict(test_loader, model)\n",
    "    test_preds['predict_price'] += predictions.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save predictions to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_names</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>image_path</th>\n",
       "      <th>islogo</th>\n",
       "      <th>fold</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clear Complexion Spot Treatment</td>\n",
       "      <td>Merle Norman</td>\n",
       "      <td>20.00</td>\n",
       "      <td>images/skin_care/Clear-Complexion-Spot-Treatme...</td>\n",
       "      <td>0.963436</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acne Solutions Emergency Gel Lotion</td>\n",
       "      <td>Clinique</td>\n",
       "      <td>17.00</td>\n",
       "      <td>images/skin_care/Acne-Solutions-Emergency-Gel-...</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>0</td>\n",
       "      <td>26.361757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RESIST Daily Pore-Refining Solution 2% BHA</td>\n",
       "      <td>Paula's Choice Skincare</td>\n",
       "      <td>33.00</td>\n",
       "      <td>images/skin_care/RESIST-Daily-Pore-Refining-So...</td>\n",
       "      <td>0.567055</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Max Complexion Correction Pads</td>\n",
       "      <td>Peter Thomas Roth</td>\n",
       "      <td>40.00</td>\n",
       "      <td>images/skin_care/Max-Complexion-Correction-Pad...</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>-1</td>\n",
       "      <td>26.567450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naturals Acne Spot Treatment</td>\n",
       "      <td>Neutrogena</td>\n",
       "      <td>8.49</td>\n",
       "      <td>images/skin_care/Naturals-Acne-Spot-Treatment_...</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>1</td>\n",
       "      <td>22.316217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                product_names                    brand  price  \\\n",
       "0             Clear Complexion Spot Treatment             Merle Norman  20.00   \n",
       "1         Acne Solutions Emergency Gel Lotion                 Clinique  17.00   \n",
       "2  RESIST Daily Pore-Refining Solution 2% BHA  Paula's Choice Skincare  33.00   \n",
       "3              Max Complexion Correction Pads        Peter Thomas Roth  40.00   \n",
       "4                Naturals Acne Spot Treatment               Neutrogena   8.49   \n",
       "\n",
       "                                          image_path    islogo  fold  \\\n",
       "0  images/skin_care/Clear-Complexion-Spot-Treatme...  0.963436     2   \n",
       "1  images/skin_care/Acne-Solutions-Emergency-Gel-...  0.011180     0   \n",
       "2  images/skin_care/RESIST-Daily-Pore-Refining-So...  0.567055    -1   \n",
       "3  images/skin_care/Max-Complexion-Correction-Pad...  0.005085    -1   \n",
       "4  images/skin_care/Naturals-Acne-Spot-Treatment_...  0.000562     1   \n",
       "\n",
       "   predicted_price  \n",
       "0              NaN  \n",
       "1        26.361757  \n",
       "2              NaN  \n",
       "3        26.567450  \n",
       "4        22.316217  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds['predict_price'] /= n_folds\n",
    "preds = oof_preds.append(test_preds)\n",
    "preds['image_path'] = preds['image_path'].apply(lambda x: x[len('../data_cleaning/'):])\n",
    "preds = preds.set_index('image_path')\n",
    "df['predicted_price'] = df['image_path'].map(preds['predict_price'])\n",
    "df.to_csv('image_prediction.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
